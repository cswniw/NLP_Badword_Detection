{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1qPeOGVfkb_",
        "outputId": "1c4340b3-2622-4aa6-c67f-012a0185c17a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 28 15:15:34 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Bert_Gpt_Chatbot\n",
            "Processing /C:/ci/aiohttp_1637857239634/work\n",
            "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/C:/ci/aiohttp_1637857239634/work'\n",
            "\u001b[0m\n",
            "Collecting kobert_tokenizer\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-0jsr3ei7/kobert-tokenizer_70aedfb181324d95b74010002abed488\n",
            "  Running command git clone -q https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-0jsr3ei7/kobert-tokenizer_70aedfb181324d95b74010002abed488\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n",
            "Requirement already satisfied: transformers==4.16.2 in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.4.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.11.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.11.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (1.21.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (0.0.47)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.16.2) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.16.2) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls drive/'My Drive'/'Colab Notebooks'//bad_chatbot\n",
        "!pip install -r drive/'My Drive'/'Colab Notebooks'/bad_chatbot/Bert_Gpt_Chatbot/requirements.txt\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
        "import sys\n",
        "sys.path.append('drive/My Drive')\n",
        "sys.path.append('drive/My Drive/Colab Notebooks/')\n",
        "sys.path.append('drive/My Drive/Colab Notebooks/bad_chatbot')\n",
        "!pip install sentencepiece\n",
        "!pip install transformers==4.16.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "88ApclaNjVvh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "from Bert_Gpt_Chatbot.dataloader.dataloader import GPT2Dataset\n",
        "from Bert_Gpt_Chatbot.model.kogpt2_model import GPT2Chat\n",
        "torch.cuda.is_available()\n",
        "ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(ctx)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_path='drive/My Drive/Colab Notebooks/bad_chatbot/Bert_Gpt_Chatbot'\n",
        "\n",
        "chatbot_file = pd.read_csv(f'{root_path}/input/datasets/KoGPT_chatbot_30000.csv')\n",
        "chatbot_file.dropna(inplace=True)\n",
        "chatbot_file.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3tJOafq5sO_",
        "outputId": "4e08a35d-0d7e-4712-f0f4-0ded97c3bd76"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 30260 entries, 0 to 30263\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   Q       30260 non-null  object\n",
            " 1   A       30260 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 709.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9vwUZQ9j0DN",
        "outputId": "6a0278f8-f040-4286-e4f6-fe6892041216"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
            "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "\n",
        "epochs = 1\n",
        "batch_size = 8\n",
        "Sneg = -1e18\n",
        "learning_rate = 3e-5\n",
        "\n",
        "model = GPT2Chat()\n",
        "model.to(device)\n",
        "\n",
        "train_dataset = GPT2Dataset(chatbot_file)\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    data = [item[0] for item in batch]\n",
        "    mask = [item[1] for item in batch]\n",
        "    label = [item[2] for item in batch]\n",
        "    return torch.LongTensor(data), torch.LongTensor(mask), torch.LongTensor(label)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, shuffle=True, collate_fn=collate_batch,)\n",
        "\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path =f\"{root_path}/output\"\n",
        "save_ckpt_path = f'{checkpoint_path}/model/cp_chatbot.pt'\n",
        "\n",
        "pre_epoch, pre_loss, train_step = 0, 0, 0\n",
        "if os.path.isfile(save_ckpt_path):\n",
        "    checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "    \n",
        "    pre_epoch = checkpoint['Epoch']\n",
        "    model.load_state_dict(checkpoint['State_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "    # pre_loss = checkpoint['Loss']\n",
        "    # train_step =  checkpoint['Train_no']\n",
        "    # total_train_step =  checkpoint['Total_train_step']\n",
        "\n",
        "    print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}\")  #, loss={pre_loss}\\n\")\n",
        "    # best_epoch += 1"
      ],
      "metadata": {
        "id": "KpDOt1ig6J1M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minimum_loss = 100\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    for batch_idx, (token_ids, mask, label) in enumerate(tqdm(train_dataloader)):\n",
        "        token_ids = token_ids.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(token_ids).logits\n",
        "        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)\n",
        "        mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out))\n",
        "        loss = criterion(mask_out.transpose(2, 1), label)\n",
        "        # 평균 loss 만들기 avg_loss[0] / avg_loss[1] <- loss 정규화\n",
        "        avg_loss = loss.sum() / mask.sum()\n",
        "        avg_loss_2 = avg_loss\n",
        "        avg_loss.backward()\n",
        "        # 학습 끝\n",
        "        optimizer.step()\n",
        "    \n",
        "    \n",
        "    state = {'Epoch': epoch,\n",
        "             'State_dict': model.state_dict(),\n",
        "             'optimizer': optimizer.state_dict()}\n",
        "    torch.save(state, f'{checkpoint_path}/model/cp_chatbot.pt')\n",
        "    print('epoch:', epoch, 'loss:', avg_loss)\n",
        "\n",
        "    if avg_loss_2 < minimum_loss :\n",
        "        model.eval()\n",
        "        torch.save(model, f'{checkpoint_path}/model/KoGPT_chatbot_{avg_loss_2}.pt')\n",
        "        minimum_loss = avg_loss_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAcJKPhQ7apd",
        "outputId": "f3b8abd5-f1e1-4ea1-fbbc-04d62cd73f07"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/3783 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "100%|██████████| 3783/3783 [12:20<00:00,  5.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 loss: tensor(20.1814, device='cuda:0', grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JzY2OujT7bch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nlDDRyEp7bZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 참고 코드\n",
        "\n",
        "# checkpoint_path =f\"{root_path}/checkpoint\"\n",
        "# save_ckpt_path = f\"{checkpoint_path}/test_GPT.pth\"\n",
        "\n",
        "\n",
        "# n_epoch = 5         # Num of Epoch\n",
        "# batch_size = 1      # 배치 사이즈\n",
        "# ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# device = torch.device(ctx)\n",
        "# save_step = 100 # 학습 저장 주기\n",
        "\n",
        "# learning_rate = 5e-5  # Learning Rate\n",
        "\n",
        "# dataset= WellnessAutoRegressiveDataset(data_path)\n",
        "# train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# model = DialogKoGPT2()\n",
        "# model.to(device)\n",
        "\n",
        "\n",
        "# loss_fct = torch.nn.CrossEntropyLoss(ignore_index=3)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# ###################\n",
        "# pre_epoch, pre_loss, train_step = 0, 0, 0\n",
        "# if os.path.isfile(save_ckpt_path):\n",
        "#     checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "#     pre_epoch = checkpoint['epoch']\n",
        "#     pre_loss = checkpoint['loss']\n",
        "#     train_step =  checkpoint['train_no']\n",
        "#     total_train_step =  checkpoint['total_train_step']\n",
        "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "#     print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}\")  #, loss={pre_loss}\\n\")\n",
        "#     # best_epoch += 1\n",
        "\n",
        "\n",
        "# ###################\n",
        "\n",
        "# losses =[]\n",
        "# for epoch in range(n_epoch):\n",
        "#     count = 0\n",
        "#     with tqdm(total=len(train_loader), desc=f\"Train({epoch})\") as pbar:\n",
        "#         for i, data in enumerate(train_loader):\n",
        "#             optimizer.zero_grad()\n",
        "#             data = torch.stack(data)  # list of Tensor로 구성되어 있기 때문에 list를 stack을 통해 변환해준다.\n",
        "#             data = data.transpose(1, 0)\n",
        "#             data= data.to(ctx)\n",
        "\n",
        "#             outputs = model(data, labels=data)\n",
        "#             _, logits = outputs[:2]\n",
        "\n",
        "#             shift_logits = logits[..., :-1, :].contiguous()\n",
        "#             shift_labels = data[..., 1:].contiguous()\n",
        "\n",
        "#             loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             losses.append(loss.item())\n",
        "\n",
        "\n",
        "#             # if count % 10 == 0:\n",
        "#             #     print('epoch no.{} train no.{}  loss = {}'.format(epoch, count + 1, loss))\n",
        "#             if (count > 0 and count % save_step == 0) or (len(data) < batch_size):\n",
        "#                 torch.save({\n",
        "#                     'epoch': epoch,\n",
        "#                     'train_no': count,\n",
        "#                     'model_state_dict': model.state_dict(),\n",
        "#                     'optimizer_state_dict': optimizer.state_dict(),\n",
        "#                     'total_train_step': len(train_loader),\n",
        "#                     'loss': loss\n",
        "#                 }, save_ckpt_path)\n",
        "#             count += 1\n",
        "#             pbar.update(1)\n",
        "#             pbar.set_postfix_str(f\"Loss: {loss.item():.3f} ({np.mean(losses):.3f})\")\n",
        "\n",
        "#             # if count % 1000 == 0:\n",
        "#             #     print('epoch no.{} train no.{}  loss = {}'.format(epoch, count + 1, loss))\n",
        "#             # if (count > 0 and count % save_step == 0) or (len(data) < batch_size):\n",
        "#             #     torch.save({\n",
        "#             #         'epoch': epoch,\n",
        "#             #         'train_no': count,\n",
        "#             #         'model_state_dict': model.state_dict(),\n",
        "#             #         'total_train_step': len(train_loader),\n",
        "#             #         'optimizer_state_dict': optimizer.state_dict(),\n",
        "#             #         'loss': loss\n",
        "#             #     }, save_ckpt_path)\n",
        "            \n",
        "           \n",
        "\n",
        "# ########################################################\n",
        "# torch.save({\n",
        "#     'epoch': epoch,\n",
        "#     'train_no': count,\n",
        "#     'model_state_dict': model.state_dict(),\n",
        "#     'total_train_step': len(train_loader),\n",
        "#     'optimizer_state_dict': optimizer.state_dict(),\n",
        "#     'loss': loss\n",
        "#     }, save_ckpt_path)\n",
        "# ########################################################"
      ],
      "metadata": {
        "id": "Kyh3Uzscafj-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "colab_train_KoGPT2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}