{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab_train_kCbert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls drive/'My Drive'/'Colab Notebooks'//bad_chatbot\n",
        "!pip install -r drive/'My Drive'/'Colab Notebooks'/bad_chatbot/Bert_Gpt_Chatbot/requirements.txt\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'\n",
        "import sys\n",
        "sys.path.append('drive/My Drive')\n",
        "sys.path.append('drive/My Drive/Colab Notebooks/')\n",
        "sys.path.append('drive/My Drive/Colab Notebooks/bad_chatbot')\n",
        "\n",
        "!pip install transformers==4.16.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy5GIL3O9oug",
        "outputId": "f69cf424-e702-4de7-c16a-789e33095bb3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 28 12:45:07 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Mounted at /content/drive\n",
            "Bert_Gpt_Chatbot\n",
            "Processing /C:/ci/aiohttp_1637857239634/work\n",
            "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/C:/ci/aiohttp_1637857239634/work'\n",
            "\u001b[0m\n",
            "Collecting kobert_tokenizer\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-404qzosp/kobert-tokenizer_c02f63381f304cb38380296101250e57\n",
            "  Running command git clone -q https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-404qzosp/kobert-tokenizer_c02f63381f304cb38380296101250e57\n",
            "Building wheels for collected packages: kobert-tokenizer\n",
            "  Building wheel for kobert-tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert-tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4649 sha256=21072206c718df793838df61b7f8df46bcf65e597d7367d44b412be3ef1c9cb1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pbodvlvo/wheels/10/b4/d9/cb627bbfaefa266657b0b4e8127f7bf96d27376fa1a23897b4\n",
            "Successfully built kobert-tokenizer\n",
            "Installing collected packages: kobert-tokenizer\n",
            "Successfully installed kobert-tokenizer-0.1\n",
            "Collecting transformers==4.16.2\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 77.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (4.11.1)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 75.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2) (1.21.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 70.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.16.2) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.16.2) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os, gc\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from Bert_Gpt_Chatbot.dataloader.dataloader import kcBERTDataset\n",
        "from Bert_Gpt_Chatbot.model.kCbert_model import kcBERTClassifier"
      ],
      "metadata": {
        "id": "3Wf8p7Dp9rj7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(ctx)\n",
        "\n",
        "root_path='drive/My Drive/Colab Notebooks/bad_chatbot/Bert_Gpt_Chatbot'\n",
        "\n",
        "\n",
        "dataset_train = []\n",
        "dataset_test = []\n",
        "\n",
        "df = pd.read_csv(f'{root_path}/input/datasets/upsampling_final_train_test_datasets_normalize_0228.csv',sep='|')\n",
        "\n",
        "train_df = pd.DataFrame()\n",
        "test_df = pd.DataFrame()\n",
        "categories = ['혐오', '섹슈얼', '일반_긍정']\n",
        "for i in categories:\n",
        "    filter_df = df[df['카테고리'] == i]\n",
        "    train_set, test_set = train_test_split(filter_df, test_size=0.2)  # test_size = 비율설정\n",
        "    train_df = pd.concat([train_df, train_set], ignore_index=True)\n",
        "    test_df = pd.concat([test_df, test_set], ignore_index=True)\n",
        "\n",
        "dataset_train = []\n",
        "dataset_test = []\n",
        "\n",
        "for i in range(len(train_df)) :\n",
        "    if train_df.iloc[i,1] == '일반_긍정' :\n",
        "        train_df.iloc[i,1] = '0'\n",
        "    elif train_df.iloc[i,1] == '섹슈얼' :\n",
        "        train_df.iloc[i,1] = '1'\n",
        "    else : train_df.iloc[i,1] = '2'\n",
        "\n",
        "    dataset_train.append([train_df.iloc[i,2], train_df.iloc[i,1]])\n",
        "\n",
        "for i in range(len(test_df)) :\n",
        "    if test_df.iloc[i,1] == '일반_긍정' :\n",
        "        test_df.iloc[i,1] = '0'\n",
        "    elif test_df.iloc[i,1] == '섹슈얼' :\n",
        "        test_df.iloc[i,1] = '1'\n",
        "    else : test_df.iloc[i,1] = '2'\n",
        "\n",
        "    dataset_test.append([test_df.iloc[i,2], test_df.iloc[i,1]])\n",
        "\n",
        "\n",
        "print(len(dataset_train))\n",
        "print(len(dataset_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRPOXYCD9rhO",
        "outputId": "1ab103c9-932e-4f01-acc7-e3c3a8b28e8a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5394\n",
            "1350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 64\n",
        "batch_size = 8\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 1\n",
        "max_grad_norm = 1\n",
        "learning_rate = 5e-5\n",
        "add_token = ['ㄱ', 'ㄴ', 'ㄷ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅅ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ',\n",
        "             'ㄲ', 'ㄸ', 'ㅃ', 'ㅆ', 'ㅉ', 'ㄳ', 'ㄵ', 'ㄶ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅄ']\n",
        "\n",
        "\n",
        "train_dataset = kcBERTDataset(dataset_train, add_token=add_token)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
        "\n",
        "test_dataset = kcBERTDataset(dataset_test, add_token=add_token)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=0, shuffle=True)\n",
        "\n",
        "model = kcBERTClassifier(vocab_size=train_dataset.vocab_size, add_token=train_dataset.added_token_num)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "t_total = len(train_dataloader) * num_epochs\n",
        "warmup_step = int(t_total * warmup_ratio)\n",
        "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr5nd235o-gA",
        "outputId": "32b6a1e8-1be9-4cf4-fc8f-a25c93104d1b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at beomi/kcbert-large were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path =f\"{root_path}/output\"\n",
        "save_ckpt_path = f'{checkpoint_path}/model/test_checkpoint_kCbert.pt'\n",
        "\n",
        "pre_epoch, pre_loss, train_step = 0, 0, 0\n",
        "if os.path.isfile(save_ckpt_path):\n",
        "    checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "    \n",
        "    pre_epoch = checkpoint['Epoch']\n",
        "    model.load_state_dict(checkpoint['State_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "    # pre_loss = checkpoint['Loss']\n",
        "    # train_step =  checkpoint['Train_no']\n",
        "    # total_train_step =  checkpoint['Total_train_step']\n",
        "\n",
        "    print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}\")  #, loss={pre_loss}\\n\")\n",
        "    # best_epoch += 1"
      ],
      "metadata": {
        "id": "b34oQWkJ9rWE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(X,Y):\n",
        "    max_vals, max_indices = torch.max(X, 1)\n",
        "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
        "    return train_acc\n",
        "\n",
        "\n",
        "best_model = 0"
      ],
      "metadata": {
        "id": "sktfqmdaprO_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "\n",
        "    train_acc = 0.0\n",
        "    test_acc = 0.0\n",
        "\n",
        "    model.train()\n",
        "    for batch_id, (input_ids, token_type_ids, attention_mask, label) in enumerate(tqdm(train_dataloader)):\n",
        "        input_ids = input_ids.long().to(device)\n",
        "        token_type_ids = token_type_ids.long().to(device)\n",
        "        attention_mask = attention_mask.long().to(device)\n",
        "        label = label.long().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(input_ids, token_type_ids, attention_mask)\n",
        "\n",
        "        del input_ids\n",
        "        del token_type_ids\n",
        "        del attention_mask\n",
        "\n",
        "\n",
        "        loss = loss_fn(out, label)\n",
        "        del label\n",
        "        del out\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    print(\"epoch {} train acc {}\".format(epoch+1, train_acc / (batch_id+1)))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad() :\n",
        "        for batch_id, (input_ids, token_type_ids, attention_mask, label) in enumerate(tqdm(test_dataloader)):\n",
        "            input_ids = input_ids.long().to(device)\n",
        "            token_type_ids = token_type_ids.long().to(device)\n",
        "            attention_mask = attention_mask.long().to(device)\n",
        "            label = label.long().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out = model(input_ids, token_type_ids, attention_mask)\n",
        "\n",
        "            test_acc += calc_accuracy(out, label)\n",
        "\n",
        "    print(\"epoch {} test acc {}\".format(epoch + 1, test_acc / (batch_id + 1)))\n",
        "\n",
        "\n",
        "    if best_model < test_acc:\n",
        "        \n",
        "        state = {'Epoch': epoch,\n",
        "                 'State_dict': model.state_dict(),\n",
        "                 'optimizer': optimizer.state_dict()}\n",
        "        torch.save(state, f'{checkpoint_path}/cp_kCbert.pt')         \n",
        "        torch.save(model, f'{checkpoint_path}/kCbert_3multi_{test_acc/(batch_id + 1)}.pt')\n",
        "\n",
        "        best_model = test_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEUlWuUuACWA",
        "outputId": "cc3e27b3-da58-4959-c347-47ab1c85cf6e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 675/675 [04:49<00:00,  2.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 train acc 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169/169 [00:19<00:00,  8.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 test acc 0.897189349112426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xdvPsSh6ACTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EhVJnM2jACQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TS7um4UEsGdi"
      },
      "outputs": [],
      "source": [
        "### 코드 참조\n",
        "\n",
        "# import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "# from IPython.display import display\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# import torch\n",
        "# from transformers import AdamW\n",
        "# from torch.utils.data import dataloader\n",
        "# from final_project.dataloader.wellness import WellnessTextClassificationDataset\n",
        "# from final_project.model.kobert import KoBERTforSequenceClassfication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9vwUZQ9j0DN"
      },
      "outputs": [],
      "source": [
        "# root_path='drive/My Drive/Colab Notebooks/final_project'\n",
        "# data_path = f\"{root_path}/data/test_KOBERT.txt\"\n",
        "# checkpoint_path =f\"{root_path}/checkpoint\"\n",
        "# save_ckpt_path = f\"{checkpoint_path}/test_KOBERT.pth\"\n",
        "\n",
        "# n_epoch = 10          # Num of Epoch\n",
        "# batch_size = 8      # 배치 사이즈\n",
        "# ctx = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# device = torch.device(ctx)\n",
        "# save_step = 100 # 학습 저장 주기\n",
        "# learning_rate = 5e-6  # Learning Rate\n",
        "\n",
        "# # WellnessTextClassificationDataset 데이터 로더\n",
        "# dataset = WellnessTextClassificationDataset(file_path=data_path, device=device)\n",
        "# train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# model = KoBERTforSequenceClassfication()\n",
        "# model.to(device)\n",
        "\n",
        "# # Prepare optimizer and schedule (linear warmup and decay)\n",
        "# no_decay = ['bias', 'LayerNorm.weight']\n",
        "# optimizer_grouped_parameters = [\n",
        "#     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "#       'weight_decay': 0.01},\n",
        "#     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "# ]\n",
        "# optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
        "\n",
        "# pre_epoch, pre_loss, train_step = 0, 0, 0\n",
        "# if os.path.isfile(save_ckpt_path):\n",
        "#     checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "#     pre_epoch = checkpoint['epoch']\n",
        "#     # pre_loss = checkpoint['loss']\n",
        "#     train_step =  checkpoint['train_step']\n",
        "#     total_train_step =  checkpoint['total_train_step']\n",
        "\n",
        "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "#     print(f\"load pretrain from: {save_ckpt_path}, epoch={pre_epoch}\")  #, loss={pre_loss}\\n\")\n",
        "#     # best_epoch += 1\n",
        "\n",
        "# losses = []\n",
        "# offset = pre_epoch\n",
        "# for step in range(n_epoch):\n",
        "#     epoch = step + offset\n",
        "#     loss = train(device, epoch, model, optimizer, train_loader, save_step, save_ckpt_path, train_step)\n",
        "#     losses.append(loss)\n",
        "\n",
        "# # data\n",
        "# data = {\n",
        "#     \"loss\": losses\n",
        "# }\n",
        "# df = pd.DataFrame(data)\n",
        "# display(df)\n",
        "\n",
        "# # graph\n",
        "# plt.figure(figsize=[12, 4])\n",
        "# plt.plot(losses, label=\"loss\")\n",
        "# plt.legend()\n",
        "# plt.xlabel('Epoch')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jqQMgbUXYCum"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}